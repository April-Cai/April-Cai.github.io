<!DOCTYPE html><html lang="en" itemscope itemtype="http://schema.org/WebPage"><head><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><title>Paper collection | Typing</title><meta name=description content=""><meta name="robots" content="index, follow"><meta name=viewport content="width=device-width, initial-scale=1"><meta http-equiv="cache-control" content="public"/><meta http-equiv="pragma" content="public"><meta name="keywords" content="Typing, Theme, Jekyll" /><meta name="author" content="Typing Theme" /><meta property="og:locale" content="en" /><meta property="og:site_name" content="Typing" /><meta property="og:type" content="WebSite" /><meta property="og:url" content="https://april-cai.github.io/blog/paper/" /><meta property="og:description" content="Typing - A Theme for Jekyll" /><meta property="og:title" content="Paper collection - Typing" /><link rel="canonical" href="https://april-cai.github.io/blog/paper/"><link rel="alternate" type="application/rss+xml" title="Typing" href="https://april-cai.github.io/feed.xml"> <script type="application/ld+json"> {"@context": "http://schema.org", "@type": "WebSite", "name": "Typing Theme", "headline": "Typing", "author": {"@type": "Person", "name": {"Typing"}}, "description": "Typing - A Theme for Jekyll", "url": "https://april-cai.github.io"} </script> <script type="text/javascript"> var GLOBAL_BASEURL = "https://april-cai.github.io"; </script><link rel="icon" type="image/png" sizes="32x32" href="https://april-cai.github.io/assets/images/favicon/favicon-32x32.png"><link rel="stylesheet" href="https://april-cai.github.io/assets/vendor-off/bootstrap/css/bootstrap.min.css"><link rel="stylesheet" href="https://april-cai.github.io/assets/vendor-off/font-awesome/css/font-awesome.min.css"><link rel="stylesheet" href="https://april-cai.github.io/assets/vendor-off/fonts/space-mono/space-mono.min.css"><link rel="stylesheet" href="https://april-cai.github.io/assets/vendor-off/fonts/gloria-hallelujah/gloria-hallelujah.min.css"><link rel="stylesheet" href="https://april-cai.github.io/assets/stylesheets/main.css"></head><body><div class="main" id="top"><div class="container wrapper"><div class="row"><div class="col-sm-3 sidebar"><div class="row avatar"> <a href="https://april-cai.github.io/"> <img class="img-responsive center-block avatar-img" src="https://april-cai.github.io/assets/images/avatar/typing.svg" height="165" width="165" alt="Typing"> </a></div><div class="row header"><h2><a href="https://april-cai.github.io/">This is <br> April Cai. </a></h2></div><div class="row menu"><ul><li> <i class="fa fa-home" aria-hidden="true"></i>&nbsp;<a href="https://april-cai.github.io/">Hello</a></li><hr class="breakline"><li class="li-fa-edit"> <i style="font-size: ;" class="fa fa-edit" aria-hidden="true"></i>&nbsp;<a href="https://april-cai.github.io/blog/" >Blog</a></li><li class="li-fa-search"> <i style="font-size: ;" class="fa fa-search" aria-hidden="true"></i>&nbsp;<a href="https://april-cai.github.io/blog/search/" >Search</a></li><li class="li-fa-tags"> <i style="font-size: ;" class="fa fa-tags" aria-hidden="true"></i>&nbsp;<a href="https://april-cai.github.io/blog/tags/" >Tags</a></li><li class="li-fa-briefcase"> <i style="font-size: ;" class="fa fa-briefcase" aria-hidden="true"></i>&nbsp;<a href="https://april-cai.github.io/projects/" onclick="ga('send', 'event', 'linkTo', 'https://april-cai.github.io/projects/', 'Projects');" >Projects</a></li><li class="li-fa-file-text-o"> <i style="font-size: ;" class="fa fa-file-text-o" aria-hidden="true"></i>&nbsp;<a href="https://april-cai.github.io/resume/" onclick="ga('send', 'event', 'linkTo', 'https://april-cai.github.io/resume/', 'Resume');" >Resume</a></li><li class="li-fa-envelope"> <i style="font-size: ;" class="fa fa-envelope" aria-hidden="true"></i>&nbsp;<a href="https://april-cai.github.io/contact/" >Contact</a></li></ul><div class="socials"><div class="row"><p></p><p>Follow Me!</p><div id="github" class="col-sm-3 columns"> <a href="https://github.com/April-Cai" target="_blank" title="GitHub" onclick="ga('send', 'event', 'linkTo', 'GitHub', 'Social Network', {useBeacon: true});" > <i class="fa fa-github icon" aria-hidden="true"></i> </a></div><div id="twitter" class="col-sm-3 columns"> <a href="https://twitter.com/GarciaCai007" target="_blank" title="Twitter" onclick="ga('send', 'event', 'linkTo', 'Twitter', 'Social Network', {useBeacon: true});" > <i class="fa fa-twitter icon" aria-hidden="true"></i> </a></div><div id="gplus" class="col-sm-3 columns"> <a href="https://plus.google.com" target="_blank" title="Google Plus" onclick="ga('send', 'event', 'linkTo', 'Google Plus', 'Social Network', {useBeacon: true});" > <i class="fa fa-google-plus icon" aria-hidden="true"></i> </a></div><div id="linkedin" class="col-sm-3 columns"> <a href="https://linkedin.com" target="_blank" title="LinkedIn" onclick="ga('send', 'event', 'linkTo', 'LinkedIn', 'Social Network', {useBeacon: true});" > <i class="fa fa-linkedin icon" aria-hidden="true"></i> </a></div></div></div></div></div><div class="col-sm-9 content-main"><article class="post" itemscope itemtype="http://schema.org/BlogPosting"><div class="row title"><h1>Paper collection</h1></div><div class="row meta" datetime="2017-12-03"> <time class="col-sm-6 datetime"> <i class="fa fa-calendar" aria-hidden="true"></i><span class="text tmargin "> 03 Dezembro, 2017 </span> </time></div><div class="row tag-list"> <i class="fa fa-tags" aria-hidden="true"></i> <a href="https://april-cai.github.io/blog/tags//research">research</a></div><div class="row"> Read this in "about 12 minutes".</div><div class="row content"><ul id="markdown-toc"><li><a href="#nerank-a-graph-based-approach-for-entity-ranking-in-document-collections" id="markdown-toc-nerank-a-graph-based-approach-for-entity-ranking-in-document-collections">NERank+: a graph-based approach for entity ranking in document collections</a></li><li><a href="#event-phase-oriented-news-summarization" id="markdown-toc-event-phase-oriented-news-summarization">Event phase oriented news summarization</a></li><li><a href="#dkgbuilder-an-architecture-for-building-a-domain-knowledge-graph-from-scratch" id="markdown-toc-dkgbuilder-an-architecture-for-building-a-domain-knowledge-graph-from-scratch">DKGBuilder: An Architecture for Building a Domain Knowledge Graph from Scratch</a></li><li><a href="#error-link-detection-and-correction-in-wikipedia" id="markdown-toc-error-link-detection-and-correction-in-wikipedia">Error Link Detection and Correction in Wikipedia</a></li><li><a href="#learning-user-credibility-for-product-ranking" id="markdown-toc-learning-user-credibility-for-product-ranking">Learning user credibility for product ranking</a></li><li><a href="#event-phase-extraction-and-summarization" id="markdown-toc-event-phase-extraction-and-summarization">Event Phase Extraction and Summarization</a></li><li><a href="#detecting-anomaly-in-data-streams-by-fractal-model" id="markdown-toc-detecting-anomaly-in-data-streams-by-fractal-model">Detecting anomaly in data streams by fractal model</a></li><li><a href="#challenges-in-chinese-knowledge-graph-construction" id="markdown-toc-challenges-in-chinese-knowledge-graph-construction">Challenges in Chinese knowledge graph construction</a></li><li><a href="#dish-comment-summarization-based-on-bilateral-topic-analysis" id="markdown-toc-dish-comment-summarization-based-on-bilateral-topic-analysis">Dish comment summarization based on bilateral topic analysis  </a></li><li><a href="#product-oriented-review-summarization-and-scoring" id="markdown-toc-product-oriented-review-summarization-and-scoring">Product-oriented review summarization and scoring</a></li><li><a href="#semi-ascalableentitymatching-system-based-on-mapreduce" id="markdown-toc-semi-ascalableentitymatching-system-based-on-mapreduce">SEMI: A Scalable Entity Matching System Based on MapReduce</a></li></ul><h3 id="nerank-a-graph-based-approach-for-entity-ranking-in-document-collections">NERank+: a graph-based approach for entity ranking in document collections</h3><p><code>14 November 2017</code><br /> Most entity ranking research aims to retrieve a ranked list of entities from a Web corpus given a user query. The rank order of entities is determined by the relevance between the query and contexts of entities. However, entities can be ranked directly based on their relative importance in a document collection, independent of any queries. In this paper, we introduce an entity ranking algorithm named NERank+. Given a document collection, NERank+ first constructs a graph model called Topical Tripartite Graph, consisting of document, topic and entity nodes. We design separate ranking functions to compute the prior ranks of entities and topics, respectively. A meta-path constrained random walk algorithm is proposed to propagate prior entity and topic ranks based on the graph model.</p><hr /><h3 id="event-phase-oriented-news-summarization">Event phase oriented news summarization</h3><p><code>17 October 2017</code><br /> Event summarization is a task to generate a single, concise textual representation of an event. This task does not consider multiple development phases in an event. However, news articles related to long and complicated events often involve multiple phases. Thus, traditional approaches for event summarization generally have difficulty in capturing event phases in summarization effectively. In this paper, we define the task of Event Phase Oriented News Summarization (EPONS). In this approach, we assume that a summary contains multiple timelines, each corresponding to an event phase. We model the semantic relations of news articles via a graph model called Temporal Content Coherence Graph. A structural clustering algorithm EPCluster is designed to separate news articles into several groups corresponding to event phases. We apply a vertex-reinforced random walk to rank news articles. The ranking results are further used to create timelines.</p><hr /><h3 id="dkgbuilder-an-architecture-for-building-a-domain-knowledge-graph-from-scratch">DKGBuilder: An Architecture for Building a Domain Knowledge Graph from Scratch</h3><p><code>22 March 2017</code><br /> In recent years, we have witnessed the technical advances in general knowledge graph construction. However, for a specific domain, harvesting precise and fine-grained knowledge is still difficult due to the long-tail property of entities and relations, together with the lack of high-quality, wide-coverage data sources. In this paper, a domain knowledge graph construction system DKGBuilder is presented. It utilizes a template-based approach to extract seed knowledge from semi-structured data. A word embedding based projection model is proposed to extract relations from text under the framework of distant supervision. We further employ an is-a relation classifier to learn a domain taxonomy using a bottom-up strategy. For demonstration, we construct a Chinese entertainment knowledge graph from Wikipedia to support several knowledge service functionalities, containing over 0.7M facts with 93.1% accuracy.</p><hr /><h3 id="error-link-detection-and-correction-in-wikipedia">Error Link Detection and Correction in Wikipedia</h3><p><code>October 24 - 28, 2016</code> <br /> The hyperlink structure of Wikipedia forms a rich semantic network connecting entities and concepts, enabling it as a valuable source for knowledge harvesting. Wikipedia, as crowd-sourced data, faces various data quality issues which significantly impacts knowledge systems depending on it as the information source. One such issue occurs when an anchor text in a Wikipage links to a wrong Wikipage, causing the error link problem. While much of previous work has focused on leveraging Wikipedia for entity linking, little has been done to detect error links. In this paper, we address the error link problem, and propose algorithms to detect and correct error links. We introduce an efficient method to generate candidate error links based on iterative ranking in an Anchor Text Semantic Network. This greatly reduces the problem space. A more accurate pairwise learning model was used to detect error links from the reduced candidate error link set, while suggesting correct links in the same time. This approach is effective when data sparsity is a challenging issue. The experiments on both English and Chinese Wikipedia illustrate the effectiveness of our approach. We also provide a preliminary analysis on possible causes of error links in English and Chinese Wikipedia.</p><hr /><h3 id="learning-user-credibility-for-product-ranking">Learning user credibility for product ranking</h3><p><code>30 September 2015</code><br /> As the explosion of user-generated data (UGC) in electronic commerce, this kind of data is scanned for trust or credibility calculation, which plays an important role in business selection. The commonly used UGC is user reviews and ratings. A new consumer without any experience with some product will read these UGCs to get an overview. However, the open and dynamic e-commerce platforms may rise the generation of unfair or deceitful reviews and ratings. Then, detecting trustful reviewers or generating authentic ratings for customers is urgent and useful. In this paper, we present a twin-bipartite graph model to catch the review and ranking relationship among users, products and shops. We design a feedback mechanism to get the consistent ranking among different level of objects, which are users and items. In the algorithm, we adjust customer credibility values by the feedback considering the rating consistency; we adjust ratings by combining customer credibility together with originally assigned ratings. We increase the credibility for a customer if the customer gives a high (low) score to a good (bad) product and decrease the value if the customer gives a low (high) score to a good (bad) product. We detect the inconsistency between semantic ratings (the review comments) and numerical ratings (scores). To deal with it, we train a classifier on the training data that are constructed automatically. The trained classifier is used to predict the semantic scores from review comments. Finally, we calculate the scores of products by considering both the customer credibility and the predicted scores. We conduct experiments using a large amount of real-world data. The experimental results show that our proposed approach provides better products ranking than the baseline systems.</p><hr /><h3 id="event-phase-extraction-and-summarization">Event Phase Extraction and Summarization</h3><p><code>02 November 2016</code><br /> Text summarization aims to generate a single, concise representation for documents. For Web applications, documents related to an event retrieved by search engines usually describe several event phases implicitly, making it difficult for existing approaches to identify, extract and summarize these phases. In this paper, we aim to mine and summarize event phases automatically from a stream of news data on the Web. We model the semantic relations of news via a graph model called Temporal Content Coherence Graph. A structural clustering algorithm EPCluster is designed to separate news articles corresponding to event phases. After that, we calculate the relevance of news articles based on a vertex-reinforced random walk algorithm and generate event phase summaries in a relevance maximum optimization framework. Experiments on news datasets illustrate the effectiveness of our approach.</p><hr /><h3 id="detecting-anomaly-in-data-streams-by-fractal-model">Detecting anomaly in data streams by fractal model</h3><p><code>10 June 2014</code><br /> Detecting anomaly in data streams attracts great attention in both academic and industry communities due to its wide range application in venture analysis, network monitoring, trend analysis and so on. However, existing methods on anomaly detection suffer three problems. 1) A large number of false positive results are generated. 2) Training data are needed to build the detection model, and an appropriate time window size along with corresponding threshold has to be set empirically. 3) Both time and space overhead is usually very high. To address these limitations. We propose a fractal-model-based approach to detection of anomalies that change underlying data distribution in this paper. Both a history-based algorithm and a parameter-free algorithm are introduced. We show that the later method consumes only limited memory and does not involve any training process. Theoretical analyses of the algorithm are presented. The experimental results on real life data sets indicate that, compared with existing anomaly detection methods, our algorithm can achieve higher precision with less space and time complexity.</p><hr /><h3 id="challenges-in-chinese-knowledge-graph-construction">Challenges in Chinese knowledge graph construction</h3><p><code>13-17 April 2015</code><br /> The automatic construction of large-scale knowledge graphs has received much attention from both academia and industry in the past few years. Notable knowledge graph systems include Google Knowledge Graph, DBPedia, YAGO, NELL, Probase and many others. Knowledge graph organizes the information in a structured way by explicitly describing the relations among entities. Since entity identification and relation extraction are highly depending on language itself, data sources largely determine the way the data are processed, relations are extracted, and ultimately how knowledge graphs are formed, which deeply involves the analysis of lexicon, syntax and semantics of the content. Currently, much progress has been made for knowledge graphs in English language. In this paper, we discuss the challenges facing Chinese knowledge graph construction because Chinese is significantly different from English in various linguistic perspectives. Specifically, we analyze the challenges from three aspects: data sources, taxonomy derivation and knowledge extraction. We also present our insights in addressing these challenges.</p><hr /><h3 id="dish-comment-summarization-based-on-bilateral-topic-analysis">Dish comment summarization based on bilateral topic analysis  </h3><p><code>13-17 April 2015</code><br /> With the prosperity of online services enabled by Web 2.0, huge amount of human generated commentary data are now available on the Internet, covering a wide range of domains on different products. Such comments contain valuable information for other customers, but are usually difficult to utilize due to the lack of common description structure, the complexity of opinion expression and fast growing data volume. Comment-based restaurant summarization is even more challenging than other types of products and services, as users’ comments on restaurants are usually mixed with opinions on different dishes but attached with only one overall evaluation score on the whole experience with the restaurants. It is thus crucial to distinguish well-made dishes from other lousy dishes by mining the comment archive, in order to generate meaningful and useful summaries for other potential customers. This paper presents a novel approach to tackle the problem of restaurant comment summarization, with a core technique on the new bilateral topic analysis model on the commentary text data. In the bilateral topic model, the attributes discussed in the comments on the dishes and the user’s evaluation on the attributes are considered as two independent dimensions in the latent space. Combined with new opinionated word extraction and clustering-based representation selection algorithms, our new analysis technique is effective to generate high-quality summary using representative snippets from the text comments. We evaluate our proposals on two real-world comment archives crawled from the most popular English and Chinese online restaurant review web sites, Yelp and Dianping. The experimental results verify the huge margin of advantage of our proposals on the summarization quality over baseline approaches in the literature.</p><hr /><h3 id="product-oriented-review-summarization-and-scoring">Product-oriented review summarization and scoring</h3><p> <code>21 January 2015</code><br /> Currently, there are many online review web sites where consumers can freely write comments about different kinds of products and services. These comments are quite useful for other potential consumers. However, the number of online comments is often large and the number continues to grow as more and more consumers contribute. In addition, one comment may mention more than one product and contain opinions about different products, mentioning something good and something bad. However, they share only a single overall score. Therefore, it is not easy to know the quality of an individual product from these comments. This paper presents a novel approach to generate review summaries including scores and description snippets with respect to each individual product. From the large number of comments, we first extract the context (snippet) that includes a description of the products and choose those snippets that express consumer opinions on them. We then propose several methods to predict the rating (from 1 to 5 stars) of the snippets. Finally, we derive a generic framework for generating summaries from the snippets. We design a new snippet selection algorithm to ensure that the returned results preserve the opinion-aspect statistical properties and attribute-aspect coverage based on a standard seat allocation algorithm. Through experimentswe demonstrate empirically that our methods are effective. We also quantitatively evaluate each step of our approach.</p><hr /><h3 id="semi-ascalableentitymatching-system-based-on-mapreduce">SEMI: A Scalable Entity Matching System Based on MapReduce</h3><p><code>28 May 2015</code><br /> MapReduce framework provides a new platform for data integration on distributed environment. We demonstrate a MapReduce-based entity resolution framework which efficiently solves the matching problem for structured, semi-structured and unstructured entities. We propose a random-based data representation method for reducing network transmission; we implement our design on MapReduce and design two solutions for reducing redundant comparisons. Our demo provides an easy-to-use platform for entity matching and performance analysis. We also compare the performance of our algorithm with the state-of-the-art blocking-based methods.</p><hr /><p><a href="http://dblp.uni-trier.de/pers/hd/h/He:Xiaofeng">REFERENCE LINK : dblp</a></p><hr style="border-top: 1px solid #9D9D9D ; color: #9D9D9D ;" class="endpost" /><div class="row sharebutton"><ul class="wrapper"><li class="title">Share</li><li class="fb"> <a href="#" title="Share on Facebook" onclick="window.open('https://www.facebook.com/sharer/sharer.php?u='+encodeURIComponent(location.href),'facebook-share-dialog','width=626,height=436');return false;">Facebook</a></li><li class="linkedin"> <a href="http://www.linkedin.com/cws/share?url=https://april-cai.github.io/blog/paper/" onclick="window.open(this.href,&quot;popupwindow&quot;,&quot;status=0,height=500,width=700,resizable=0,top=50,left=100&quot;);return false;" target="_blank" title="Share on LinkedIn">Linkedin</a></li><li class="googleplus"> <a href="https://plus.google.com/share?url=https://april-cai.github.io/blog/paper/" onclick="window.open(this.href,&quot;popupwindow&quot;,&quot;status=0,height=500,width=700,resizable=0,top=50,left=100&quot;);return false;" target="_blank" title="Share on Google Plus">Google+</a></li></ul></div><div class="day-quote"><div class="title"> Quote Day</div><div class="content"> Everything is hard before it is easy.</div></div><div class="row author"><div class="col-sm-2 image"> <img class="img-responsive center-block" src="https://april-cai.github.io/assets/images/author/typing.svg" alt=""></div><div class="col-sm-10 bio"><p class="title">Author</p><p class="name">Typing Theme</p><p class="content">Lorem ipsum dolor sit amet, consectetur adipisicing elit. Tempora non aut eos voluptas debitis unde impedit aliquid ipsa.</p></div></div><div class="disqus" id="comments"><div class="title"><h3><i class="fa fa-comments" aria-hidden="true"></i>&nbsp;The comment for this post is disabled.</h3></div></div></div></article></div></div></div><div class="footer"><div class="container-fluid"><div class="row-fluid"><div class="col-sm-4 copyright"> <span>Typing © 2016-2018 • All right reserved.</span></div><div class="col-sm-4 message"> <span>Simplicity is fashionable.</span></div><div class="col-sm-4 madeby"> <span>Made with&nbsp;<a href="https://jekyllrb.com" target="_blank">Jekyll</a>&nbsp;using&nbsp;<a href="https://github.com/williamcanin/typing-jekyll-template" target="_blank">Typing</a>&nbsp;Theme.</span></div><a class="pull-right top" href="#top"><i class="fa fa-caret-up" aria-hidden="true"></i></a></div></div></div></div><script src="https://april-cai.github.io/assets/vendor-off/jquery/js/jquery.min.js"></script> <script src="https://april-cai.github.io/assets/vendor-off/bootstrap/js/bootstrap.min.js"></script> <script src="https://april-cai.github.io/assets/javascripts/jekyll-spotify-plugin.min.js"></script> <script type="text/javascript" src="https://april-cai.github.io/assets/javascripts/post.js"></script> <script type="text/javascript"> jQuery(document).ready(function($) { $('.post > .row.content a').addClass('ga-event'); $('.page.spage > .row.content th a').addClass('ga-event'); $('.row.content li a').removeClass('ga-event'); $('a.ga-event').attr('onclick', 'ga(\'send\',\'event\',\'linkTo\',this.href,\'https://april-cai.github.io/blog/paper/\');'); }); </script> <script src="https://april-cai.github.io/assets/javascripts/global.js"></script></body></html>
